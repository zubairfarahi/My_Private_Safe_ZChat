{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10e24def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f62fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = getpass.getpass('please enter the key: ')\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fedc300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac70698e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-BrbYvhmnDdKjjFkQu6wR8hKB0bqeS', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks designed to facilitate building applications with large language models (LLMs). While they share some overlapping goals—such as enabling easier integration of LLMs into applications—they have different focuses, architectures, and use cases.\\n\\n**Key Differences Between LangChain and LlamaIndex:**\\n\\n1. **Primary Focus and Use Cases:**\\n   - **LangChain:**  \\n     - Focuses on building **conversational agents, chatbots, and complex LLM-powered workflows**.  \\n     - Provides tools for chaining multiple prompts, managing memory, handling agents that can make decisions, and orchestrating complex interactions with LLMs.  \\n     - Well-suited for end-to-end applications where multi-turn conversations, reasoning, and dynamic decision-making are needed.\\n   \\n   - **LlamaIndex (GPT Index):**  \\n     - Primarily centered on **indexing and querying large document datasets** with LLMs.  \\n     - Facilitates creating efficient retrieval systems, enabling users to query extensive knowledge bases, documents, or data repositories via natural language.  \\n     - Ideal for building question-answering systems, document retrieval, and knowledge management over large datasets.\\n\\n2. **Core Functionalities:**\\n   - **LangChain:**  \\n     - Provides **chains**—sequences of prompts and processing steps—to structure interactions.  \\n     - Supports **agents** capable of selecting actions based on inputs, with integrations for tools and APIs.  \\n     - Includes memory modules to maintain context across interactions.  \\n     - Offers a wide array of integrations with LLM providers, vector stores, and other tools.\\n   \\n   - **LlamaIndex:**  \\n     - Offers **data ingestion pipelines** to load, process, and index documents from various sources (files, databases, web).  \\n     - Implements **indexing structures** (e.g., GPT indices, trees) to optimize retrieval and querying.  \\n     - Supports **query-over-index** functionalities, enabling natural language questions over large datasets.\\n     - Focuses on enabling LLMs to access and reason over structured or unstructured data efficiently.\\n\\n3. **Architectural Approach:**\\n   - **LangChain:**  \\n     - Emphasizes **modular component composition**—promoting flexible workflows.  \\n     - Can be seen as a **framework for developing multi-step LLM applications** with clear abstractions for prompts, memory, tools, and agents.\\n   \\n   - **LlamaIndex:**  \\n     - Acts as a **data layer** or middleware that constructs intelligent indices over data sources for efficient retrieval and Q&A tasks.  \\n     - Focuses on **building data structures** that facilitate querying large datasets with minimal latency.\\n\\n4. **Community and Ecosystem:**\\n   - Both have active communities, but LangChain tends to be more widely adopted for diverse applications, particularly those involving complex conversational flows and agent-based architectures.\\n   - LlamaIndex is often chosen for projects that involve **large-scale document indexing and question-answering**.\\n\\n---\\n\\n### **Summary Table**\\n\\n| Aspect                     | **LangChain**                             | **LlamaIndex (GPT Index)**                 |\\n|----------------------------|-------------------------------------------|-------------------------------------------|\\n| Primary Focus              | Building chatbots, conversational agents, complex workflows | Indexing and querying large datasets   |\\n| Core Use Cases             | Multi-turn conversations, agents, chaining | Document retrieval, knowledge bases    |\\n| Functionalities            | Chains, Agents, Memory, Tool integration | Data ingestion, indexing, query-over-data |\\n| Architectural Approach     | Modular, orchestrates multi-step workflows | Builds indices for efficient retrieval   |\\n| Suitable For               | Chatbots, assistants, reasoning workflows | Q&A systems, document search, knowledge retrieval |\\n\\n---\\n\\n### **In Summary:**\\n- **LangChain** is ideal if you're developing applications that require complex interactions, multi-step reasoning, or conversational AI.\\n- **LlamaIndex** is best suited for building systems that require efficient access and querying of large document collections or data repositories through natural language.\\n\\nBoth can sometimes be used together—for example, using LlamaIndex to handle data retrieval and LangChain to manage conversation logic—forming powerful hybrid systems.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1752115473, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=852, prompt_tokens=19, total_tokens=871, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
    "\n",
    "client.chat.completions.create(\n",
    "    model='gpt-4.1-nano',\n",
    "    messages=[{\n",
    "        'role' : 'user',\n",
    "        'content' : YOUR_PROMPT\n",
    "    }]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21b11df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
    "    return client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "def system_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"developer\", \"content\": message}\n",
    "\n",
    "def assistant_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"assistant\", \"content\": message}\n",
    "\n",
    "def user_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"user\", \"content\": message}\n",
    "\n",
    "def pretty_print(message: str) -> str:\n",
    "    display(Markdown(message.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0496ec5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LangChain and LlamaIndex (formerly known as GPT Index) are both tools designed to facilitate working with large language models (LLMs) and building AI-powered applications, but they serve different primary purposes and have distinct features. Here's a comparison to clarify their differences:\n",
       "\n",
       "**LangChain**\n",
       "\n",
       "- **Purpose:** A comprehensive framework for building, managing, and deploying language model-based applications.\n",
       "- **Focus:** Facilitates chaining multiple prompts, integrating external tools, managing conversation state, and handling complex workflows.\n",
       "- **Features:**\n",
       "  - Modular components for prompt management, memory, agents, and tool integrations.\n",
       "  - Supports building chatbots, question-answering systems, and other applications that require multi-step reasoning.\n",
       "  - Provides infrastructure for managing prompts, outputs, and data flows.\n",
       "  - Extensible with custom components and integrations.\n",
       "- **Use Cases:** Conversational AI, complex multi-step workflows, integrations with APIs and tools.\n",
       "\n",
       "**LlamaIndex (GPT Index)**\n",
       "\n",
       "- **Purpose:** Simplifies the process of indexing, querying, and managing large document corpora to enable efficient retrieval-augmented generation (RAG) systems.\n",
       "- **Focus:** Building indices over unstructured data to facilitate fast retrieval and question-answering via LLMs.\n",
       "- **Features:**\n",
       "  - Supports multiple index types (e.g., tree, list, keyword, vector) tailored for different retrieval needs.\n",
       "  - Allows ingesting various data sources (documents, PDFs, databases).\n",
       "  - Optimizes LLM use by retrieving relevant information from large datasets before generating responses.\n",
       "- **Use Cases:** Building knowledge bases, document retrieval systems, chatbots that reference large datasets.\n",
       "\n",
       "---\n",
       "\n",
       "**Summary**\n",
       "\n",
       "- **LangChain** is a flexible pipeline and workflow framework for building LLM applications, emphasizing prompt management, chaining, and tool integration.\n",
       "- **LlamaIndex** focuses on creating efficient data structures (indices) over large unstructured datasets to enable retrieval-based question-answering.\n",
       "\n",
       "**In practice:**  \n",
       "You might use **LlamaIndex** to create an index over your document set so that your application can quickly retrieve relevant info, and then employ **LangChain** to orchestrate prompts, manage conversations, and integrate retrieval with other tools or APIs for a complete solution.\n",
       "\n",
       "---\n",
       "\n",
       "**Note:** Both projects are open-source and actively developed, often used together in complex AI applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [user_prompt(YOUR_PROMPT)]\n",
    "chatgpt_getresponse = get_response(client, messages)\n",
    "pretty_print(chatgpt_getresponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be61c35d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Are you kidding me? I don't have time for this triviality when I'm starving and furious! But since you ask, crushed ice — because at least then I get some kind of relief from this endless wait! Honestly, make your choice quickly before I lose my mind!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    system_prompt(\"You are irate and extremely hungry.\"),\n",
    "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
    "]\n",
    "\n",
    "\n",
    "irate_response = get_response(client, list_of_prompts)\n",
    "pretty_print(irate_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29cbb724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I think crushed ice can be super refreshing, especially in a cold drink or a snow cone, giving it that nice, frosty texture. Cubed ice, on the other hand, is great for keeping drinks chilled without watering them down too quickly. Both have their own cool vibes—do you have a favorite?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
    "\n",
    "joyful_response = get_response(client, list_of_prompts)\n",
    "pretty_print(joyful_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7b55390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-Brc0TgNXSQmWbMgAteHlc9Ftep4VR', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I think crushed ice can be super refreshing, especially in a cold drink or a snow cone, giving it that nice, frosty texture. Cubed ice, on the other hand, is great for keeping drinks chilled without watering them down too quickly. Both have their own cool vibes—do you have a favorite?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1752117181, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=63, prompt_tokens=30, total_tokens=93, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "print(joyful_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6d78d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure! Here's a sentence using the words:\n",
       "\n",
       "\"During the unusual weather, the villagers noticed a peculiar stimple on the leaves, while a faint falbean scent drifted through the air.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
    "]\n",
    "\n",
    "# print(list_of_prompts)\n",
    "stimple_response = get_response(client, list_of_prompts)\n",
    "pretty_print(stimple_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f03d7d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The stimple wrench easily turned the falbean to secure the parts together."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
    "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
    "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
    "]\n",
    "\n",
    "stimple_response = get_response(client, list_of_prompts)\n",
    "pretty_print(stimple_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ff5432e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's analyze Billy's options carefully:\n",
       "\n",
       "**Option 1: Fly + Bus**\n",
       "- Flight time: 3 hours\n",
       "- Bus time after flight: 2 hours\n",
       "- Total travel time: 3 + 2 = 5 hours\n",
       "\n",
       "**Option 2: Teleporter + Bus**\n",
       "- Teleporter time: 0 hours\n",
       "- Bus time after teleporter: 1 hour\n",
       "- Total travel time: 0 + 1 = 1 hour\n",
       "\n",
       "**Current time:** 1PM local time\n",
       "\n",
       "**Goal:** Reach home **before 7PM EDT**\n",
       "\n",
       "---\n",
       "\n",
       "### Step 1: Determine elapsed time in local time\n",
       "\n",
       "- For **Option 1**:\n",
       "  - Arrival time: 1PM + 5 hours = 6PM local time\n",
       "\n",
       "- For **Option 2**:\n",
       "  - Arrival time: 1PM + 1 hour = 2PM local time\n",
       "\n",
       "---\n",
       "\n",
       "### Step 2: Convert local time to EDT\n",
       "\n",
       "Assuming Billy's current local time is **San Francisco time (PST/PDT)**, and his destination is EDT (Eastern Daylight Time).\n",
       "\n",
       "- **San Francisco** is either UTC-8 (PST) or UTC-7 (PDT).  \n",
       "- **EDT** is UTC-4.\n",
       "\n",
       "To keep it simple, let's assume it's **PST (UTC-8)** now, and during daylight saving hours (March-early November), PST is adjusted to PDT (UTC-7). For accuracy, assume **PDT (UTC-7)**:\n",
       "\n",
       "- **San Francisco current time:** 1PM PDT (UTC-7)\n",
       "- **Time difference between PDT and EDT:** EDT is UTC-4  \n",
       "  - Difference: 3 hours ahead (EDT is 3 hours ahead of PDT)\n",
       "\n",
       "---\n",
       "\n",
       "### Step 3: Calculate arrival time in EDT\n",
       "\n",
       "**Option 1:**\n",
       "- Arrival at 6PM local time (PDT)\n",
       "- Convert to EDT: add 3 hours\n",
       "- Arrival time in EDT: 6PM + 3 hours = **9PM EDT**\n",
       "\n",
       "**Option 2:**\n",
       "- Arrival at 2PM local time (PDT)\n",
       "- Convert to EDT: 2PM + 3 hours = **5PM EDT**\n",
       "\n",
       "---\n",
       "\n",
       "### **Final assessment:**\n",
       "\n",
       "- **Option 1 (Fly + Bus):** Billy arrives home at approximately **9PM EDT**, **which is after** the 7PM deadline.\n",
       "- **Option 2 (Teleporter + Bus):** Billy arrives at **5PM EDT**, **which is before** the 7PM deadline.\n",
       "\n",
       "---\n",
       "\n",
       "### **Conclusion:**\n",
       "Yes, it **does** matter which option Billy chooses. The teleporter + bus option gets him home well before the 7PM deadline, while flying + bus does not. Therefore, **Billy should take the teleporter + bus** to ensure he arrives before 7PM EDT."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reasoning_problem = \"\"\"\n",
    "Billy wants to get home from San Fran. before 7PM EDT.\n",
    "\n",
    "It's currently 1PM local time.\n",
    "\n",
    "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
    "\n",
    "Does it matter which travel option Billy selects?\n",
    "\"\"\"\n",
    "\n",
    "list_of_prompts = [\n",
    "    user_prompt(reasoning_problem)\n",
    "]\n",
    "\n",
    "reasoning_response = get_response(client, list_of_prompts)\n",
    "pretty_print(reasoning_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f680adf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
